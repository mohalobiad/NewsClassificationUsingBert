# -*- coding: utf-8 -*-
"""Multiclass news classification using Natural Language Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YYVjaQGvSHro6l-KEyktDd_lfLx0z2La

#Access to Google :
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/NewsClassification

"""#Gereken Kütüphaneleri import et"""

import sqlite3
import pandas as pd
import re
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""#The BERT Model We Will Use"""

#Bert Modeli
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-uncased")
bert = AutoModel.from_pretrained("dbmdz/bert-base-turkish-uncased")

"""#Loading and Exploring the Dataset"""

database_file = 'News.db'
conn = sqlite3.connect(database_file)
query = "SELECT * FROM Haberler"
df = pd.read_sql(query, conn)
conn.close()

df

df.info()

"""The data appears to have been successfully retrieved from News.db. As a result of running the code, the table contains a total of 53,461 records spread across 6 columns. The total data size in memory is approximately 2.4 MB. The data is ready for analysis."""

df.Baslik[1]

"""#the pre-processing method that we will use for Title Column


"""

#Pre-process
file_path = "stop.tr.turkish-lucene.txt"
with open(file_path, "r", encoding="utf-8") as file:
    stop_words = file.read().splitlines()
def clean(df):
    punctuation_no_space = re.compile(r"(\.)|(\;)|(\:)|(\!)|(\?)|(\,)|(\")|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\>)|(\<)|(\{)|(\})|(‘)|(“)|(”)|(°)|(\')")
    punctuation_with_space = re.compile(r"(<br\s/><br\s/?)|(-)|(/)|(:)")
    html_tags = r'<.*?>'
    cleaned_lines = []
    for line in df:
        line_cleaned = punctuation_no_space.sub("", line.lower())
        line_cleaned = punctuation_with_space.sub(" ", line_cleaned)
        line_cleaned = re.sub(html_tags, "", line_cleaned)
        line_cleaned = line_cleaned.split()
        line_cleaned = [word for word in line_cleaned if word not in stop_words]
        cleaned_lines.append(' '.join(line_cleaned))

    return cleaned_lines

"""The file containing Turkish stopwords (stop.tr.turkish-lucene.txt) is opened and converted into a list, which is then assigned to the variable stop_words_list. The cleaning function performs the following tasks:  

The function takes a list of texts (df) as input. Three patterns are defined to remove unwanted characters:  
 -no_space: Removes specific characters without leaving any space.  
 -with_space: Removes specific characters and replaces them with spaces.  
 -html_pattern: Removes HTML tags.  
Each text in the DataFrame is processed as follows:  
 -The text is converted to lowercase.  
 -Characters are removed according to the three defined patterns.  
 -The text is split into words.  
 -Stopword words are removed from the text.  
 -The words are joined back together to form clean texts, which are added to a temporary list.   
Finally, the function returns a new list of cleaned texts.  
"""

df['cleaned_title'] = clean(df['Baslik'])
df.head()

"""#imbalanced"""

value_counts = df['Kategori'].value_counts()

value_counts

import seaborn as sns
(value_counts.plot(kind='bar'));

"""Imbalanced datasets are a common problem in machine learning, where one class has significantly more observations than the other. This can lead to biased models and poor performance on the minority class. Several solutions have been proposed to address this issue :    
* 1 - **Undersampling**:  
** Undersampling is a technique used to reduce an imbalanced dataset by decreasing the size of the majority class while preserving all the data from the minority class. This is one of the techniques that data scientists can use to extract more accurate information from an initially imbalanced dataset. However, it has some disadvantages, such as the loss of important information. Despite this, it remains a common and important skill for data scientists.
* 2 - **Oversampling**:  
** This is used when data scientists do not have sufficient information. One class is abundant (majority), while the other is rare or scarce (minority). In oversampling, the scientist increases the number of rare events. The scientist uses a specific technique to create artificial events. One of the techniques used to create artificial events is called Synthetic Minority Over-sampling Technique (SMOTE).
* 3 - **ensemble methods for imbalanced data**
* 4 - **focal loss**  
The method we will use is **Undersampling**

##Undersampling
"""

#First, we take the classes with the most examples, which are 'yaşam', 'ekonomi', 'spor', 'sağlık
selected_categories = ['yaşam', 'ekonomi', 'spor', 'sağlık']
selected_rows = df.loc[df['Kategori'].isin(selected_categories)]
new_df = selected_rows[['Id','cleaned_title', 'Kategori']].copy()

new_df

new_df.reset_index(drop=True, inplace=True) #Reordering the index

new_df

y = new_df["Kategori"]

y = new_df["Kategori"]
y.value_counts().plot.pie(autopct='%.2f')

import imblearn
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
undersample = RandomUnderSampler(random_state=42)
X = new_df["cleaned_title"].to_numpy().reshape(-1, 1) # We convert it to an array and also change its shape
Y = new_df["Kategori"]

X_under, Y_under = undersample.fit_resample(X, Y)

print(sorted(Counter(Y_under).items()))

# Creating a new DataFrame after performing undersampling
import pandas as pd
df_under = pd.DataFrame({
    "Baslik": X_under.flatten(),
    "Kategori": Y_under
})

df_under

y = df_under["Kategori"]

y.value_counts().plot.pie(autopct='%.2f')

"""
Here we are testing, i.e., checking whether our text and label sequences were disrupted after performing undersampling.
"""
print(X_under[1])
print(Y_under[1])

search_term = str(X_under[1][0])
result = df_under.loc[df_under["Baslik"] == search_term]
print(result)
# As seen, the order has not been disrupted, the same text corresponds to the same class.

"""# Converting the 'Kategori' column into numbers"""

df_under['label_num'] = df_under['Kategori'].map({
    'ekonomi' : 0,
    'sağlık': 1,
    'spor': 2,
    'yaşam' : 3

})

search_term = str(X_under[1][0])
result = df_under.loc[df_under["Baslik"] == search_term]
print(result)
# We assigned 0 to 'ekonomi' above, and as we can see, it really has 0 in the 'label_num' column.

"""#Splitting our data into Train, Test, and Validation sets"""

x = df_under['Baslik']
y = df_under['label_num']

x

train_text, temp_text, train_labels, temp_labels = train_test_split(x, y,
                                                                    random_state=2018,
                                                                    test_size=0.3,
                                                                    stratify=y)
# Validation-Test split
val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,
                                                                random_state=2018,
                                                                test_size=0.5,
                                                                stratify=temp_labels)

print(train_text.shape)
print(temp_text.shape)
print(train_labels.shape)
print(temp_labels.shape)
print(val_text.shape)
print(test_text.shape)
print(val_labels.shape)
print(test_labels.shape)

"""#Tokenization and Feature extraction

Before starting this, we first need to specify the length of the title becausethe output vector from the tokenization step needs to have the same length.
To determine the appropriate length, we need to first visualize the distribution of words in the titles.
"""

seq_len = [len(title.split()) for title in df_under['Baslik']]
plt.xlim(0, 20)

pd.Series(seq_len).hist(bins = 200,color='firebrick')
plt.xlabel('Number of Words')
plt.ylabel('Number of texts')

"""Based on this, the average length is around 15, so we will set the length to 15."""

MAX_LENGHT = 15
# We are performing the tokenization and encoding process for the train set
tokens_train = tokenizer.batch_encode_plus(
    train_text.tolist(), #convert the train_text data to a list because the tokenizer expects a list.ز
    max_length = MAX_LENGHT, #pecify the length.
    pad_to_max_length=True, #If the length is less than 15, it will be padded with zeros.
    truncation=True,#If the length is greater than 15, it will be truncated.
    return_tensors='pt'
     #return the result as PyTorch tensors, as official models have weights compatible with PyTorch-Transformers.
     )
#perform the tokenization and encoding process for the validation set
tokens_val = tokenizer.batch_encode_plus(
    val_text.tolist(),
    max_length = MAX_LENGHT,
    pad_to_max_length=True,
    truncation=True,
    return_tensors='pt'
)
# perform the tokenization and encoding process for the test set
tokens_test = tokenizer.batch_encode_plus(
    test_text.tolist(),
    max_length = MAX_LENGHT,
    pad_to_max_length=True,
    truncation=True,
    return_tensors='pt'
)

train_y = torch.tensor(train_labels.tolist())
val_y = torch.tensor(val_labels.tolist())
test_y = torch.tensor(test_labels.tolist())

# Defining the Data Loader structure
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32
train_data = TensorDataset(tokens_train['input_ids'], tokens_train['attention_mask'], train_y)
train_sampler = RandomSampler(train_data)                     # It is used to randomly select samples from the training dataset.
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(tokens_val['input_ids'], tokens_val['attention_mask'], val_y)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)

"""#Model Building Process

##Freezing the model parameters
"""

for param in bert.parameters():
    param.requires_grad = False
# We are freezing the weights of the BERT model here. This will prevent these weights from being updated during the training process.
# The reason is that when using a pre-trained model like BERT for a new task, in this case, multi-class news classification,
# we should freeze most of the weights and only update the weights in the last layers.
# This helps prevent the model from losing the knowledge it has acquired previously and allows us to achieve good results.
# Additionally, it reduces training time, memory usage, and computer resource consumption, as well as mitigating the risk of overfitting.

"""##Our model"""

class BERT_Arch(nn.Module):
    def __init__(self, bert):
      super(BERT_Arch, self).__init__()
      self.bert = bert
      self.dropout = nn.Dropout(0.1)
      self.relu =  nn.ReLU()
      self.fc1 = nn.Linear(768,512)
      self.fc2 = nn.Linear(512,4)
      self.softmax = nn.LogSoftmax(dim=1)
    def forward(self, sent_id, mask):
      a = self.bert(sent_id, attention_mask=mask)['pooler_output']

      x = self.fc1(a)
      x = self.relu(x)
      x = self.dropout(x)
      x = self.fc2(x)
      x = self.softmax(x)
      return x


model = BERT_Arch(bert)
from transformers import AdamW
optimizer = AdamW(model.parameters(),
                  lr = 1e-5)

cross_entropy  = nn.NLLLoss()
epochs = 8
# Here, we have chosen to use the ReLU activation function in the hidden layers because it performs better and provides excellent results compared to other activation functions.
# In the output layer, we use Softmax because our output has more than two classes, and for multi-class classification tasks, we need to use an activation function like Softmax.
# we didn't used The Sigmoid function because it can only be used for binary classification.
# AdamW is considered a reliable and preferred choice by many developers due to its effectiveness.

"""## Defining the Train and Evaluate Methods"""

def train():
  model.train()
  total_loss, total_accuracy = 0, 0

  for step,batch in enumerate(train_dataloader):
    if step % 50 == 0 and not step == 0:
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))
    batch = [r for r in batch]
    sent_id, mask, labels = batch
    model.zero_grad()
    preds = model(sent_id, mask)
    loss = cross_entropy(preds, labels)
    total_loss = total_loss + loss.item()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    preds=preds.detach().cpu().numpy()

  avg_loss = total_loss / len(train_dataloader)

  return avg_loss

def evaluate():
  print("\nEvaluating...")
  model.eval()
  total_loss, total_accuracy = 0, 0
  for step,batch in enumerate(val_dataloader):
    if step % 50 == 0 and not step == 0:


      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))

    batch = [t for t in batch]
    sent_id, mask, labels = batch
    with torch.no_grad():
      preds = model(sent_id, mask)
      loss = cross_entropy(preds,labels)
      total_loss = total_loss + loss.item()
      preds = preds.detach().cpu().numpy()
  avg_loss = total_loss / len(val_dataloader)
  return avg_loss

"""#Model Results"""

weights_path = '/content/drive/MyDrive/NewsClassification/outputBert.pt'

# Train and predict
best_valid_loss = float('inf')
train_losses = []
valid_losses = []

for epoch in range(epochs):
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))
    train_loss = train()
    valid_loss = evaluate()
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), weights_path)
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')

"""#Model Performance"""

path = '/content/drive/MyDrive/NewsClassification/outputBert.pt'
model.load_state_dict(torch.load(path))

with torch.no_grad():
  preds = model(tokens_test['input_ids'], tokens_test['attention_mask'])
  preds = preds.detach().cpu().numpy()

preds = np.argmax(preds, axis = 1)
print(classification_report(test_y, preds))

precision = [0.83, 0.90, 0.97, 0.88]
recall = [0.92, 0.86, 0.95, 0.86]
f1_score = [0.87, 0.88, 0.95, 0.87]
labels = ['ekonomi 0', 'sağlık 1', 'spor 2', 'yaşam 3']

#Precision
  plt.figure(figsize=(10, 5))
  plt.bar(labels, precision, color='red', alpha=0.5, label='Precision')
  plt.xlabel('sınıflar')
  plt.ylabel('sonuç')
  plt.legend()
  plt.show()

"""Ekonomi 0:  
Precision (0.83): This means that the model correctly predicted that 83% of the instances belong to the Ekonomi  (0) category, and these instances indeed belong to that class. The remaining 17% represents instances where the model predicted them as Ekonomi  (0), but they actually did not belong to that class.  

Sağlık 1:  
Precision (0.90): This means that the model correctly predicted that 90% of the instances belong to the Sağlık  (1) category, and these instances indeed belong to that class. The remaining 10% represents instances where the model predicted them as Sağlık  (1), but they actually did not belong to that class.    

Spor 2:  
Precision (0.97): This means that the model correctly predicted that 97% of the instances belong to the Spor (2) category, and these instances indeed belong to that class. The remaining 3% represents instances where the model predicted them as Spor (2), but they actually did not belong to that class.  

Yaşam 3:    
Precision (0.88): This means that the model correctly predicted that 88% of the instances belong to the Yaşam (3) category, and these instances indeed belong to that class. The remaining 13% represents instances where the model predicted them as Yaşam (3), but they actually did not belong to that class.   
"""

#Precision
plt.figure(figsize=(10, 5))
plt.bar(labels, recall, color='gray', alpha=0.5, label='Recall')
plt.xlabel('sınıflar')
plt.ylabel('sonuç')
plt.legend()
plt.show()

"""Ekonomi 0:  
Recall (0.92): This means that the model correctly classified 92% of the instances that actually belong to the Ekonomi (0) category. However, there were still 8% of the instances that the model failed to classify correctly.  

Sağlık 1:   
Recall (0.86): This means that the model correctly classified 86% of the instances that actually belong to the Sağlık (1) category. The remaining 14% were instances that the model failed to classify correctly.     

Spor 2:   
Recall (0.94): This means that the model correctly classified 94% of the instances that actually belong to the Spor (2) category. However, 6% of the instances were misclassified by the model.  

Yaşam 3:  
Recall (0.86): This means that the model correctly classified 86% of the instances that actually belong to the Yaşam (3) category. The remaining 14% were instances that the model failed to classify correctly.   
"""

#f1-score
plt.figure(figsize=(10, 5))
plt.bar(labels, f1_score, color='blue', alpha=0.5, label='F1-Score')
plt.xlabel('sınıflar')
plt.ylabel('sonuç')
plt.legend()
plt.show()

"""Ekonomi 0:  
F1-Score (0.87): This ratio provides a balance between precision and recall and gives a comprehensive view of the model's performance in classifying Ekonomi (0). A high F1-score (0.87) indicates that the model has achieved a good balance between precision and recall, reflecting the model’s effectiveness in classifying Ekonomi (0).

Sağlık 1:   
F1-Score (0.88): This ratio provides a balance between precision and recall and gives a comprehensive view of the model's performance in classifying Sağlık (1). A high F1-score (0.88) indicates that the model has achieved a good balance between precision and recall, reflecting the model’s effectiveness in classifying Sağlık (1).

Spor 2:   
F1-Score (0.95): This ratio provides a balance between precision and recall and gives a comprehensive view of the model's performance in classifying Spor (2). A high F1-score (0.95) indicates that the model has achieved a good balance between precision and recall, reflecting the model’s effectiveness in classifying Spor (2).

Yaşam 3:      
F1-Score (0.87): This ratio provides a balance between precision and recall and gives a comprehensive view of the model's performance in classifying Yaşam (3). A high F1-score (0.87) indicates that the model has achieved a good balance between precision and recall, reflecting the model’s effectiveness in classifying Yaşam (3).  
"""

cm = confusion_matrix(test_y, preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""The model correctly classified 746 instances as Ekonomi (0).  
The model correctly classified 687 instances as Sağlık (1).   
The model correctly classified 765 instances as Spor (2).   
The model correctly classified 697 instances as Yaşam (3).   
These values represent the True Positive values in the confusion matrix.   

Based on these results, it can be seen that the model performs well in classification overall. This is supported by the high values in precision, recall, and F1-scores across all classes (0 to 3). For example, precision ranges from 0.83 to 0.97, recall ranges from 0.86 to 0.94, and the F1-score ranges from 0.87 to 0.95. These results positively indicate that the model has a strong classification capability.

#Test
"""

file_path = "/content/drive/MyDrive/NewsClassification/stop.tr.turkish-lucene.txt"
with open(file_path, "r", encoding="utf-8") as file:
    stop_words = file.read().splitlines()
def clean(df):
    punctuation_no_space = re.compile(r"(\.)|(\;)|(\:)|(\!)|(\?)|(\,)|(\")|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\>)|(\<)|(\{)|(\})|(‘)|(“)|(”)|(°)|(\')")
    punctuation_with_space = re.compile(r"(<br\s/><br\s/?)|(-)|(/)|(:)")
    html_tags = r'<.*?>'
    cleaned_lines = []
    for line in df:
        line_cleaned = punctuation_no_space.sub("", line.lower())
        line_cleaned = punctuation_with_space.sub(" ", line_cleaned)
        line_cleaned = re.sub(html_tags, "", line_cleaned)
        line_cleaned = line_cleaned.split()
        line_cleaned = [word for word in line_cleaned if word not in stop_words]
        cleaned_lines.append(' '.join(line_cleaned))

    return cleaned_lines

"""  
    'ekonomi' : 0,  
    'sağlık': 1,  
    'spor': 2,  
    'yaşam' : 3  
  
  
"""

testText = ["Sinop'ta soğuk hava depolarından çıkarılan toriklerden lakerda yapımına başlandı", #yaşam (3)
                    "KDV istismarını önleyecek düzenleme haksız rekabetin önüne geçecek",#ekonomi (0)
                    "Ünlü teknik direktör, şampiyon yaptığı takıma başkan oldu",#sport (2)
                    "Türk doktorun “Fındık tekniği” yöntemi dünya tıp literatürüne girdi",#sağlık (1)
                    ]
testText = clean(testText)

MAX_LENGHT = 15
tokens_unseen = tokenizer.batch_encode_plus(
    testText,
    max_length = MAX_LENGHT,
    pad_to_max_length=True,
    truncation=True,
    return_tensors='pt'
)

with torch.no_grad():
  preds = model(tokens_unseen['input_ids'],tokens_unseen['attention_mask'])
  preds = preds.detach().cpu().numpy()

preds = np.argmax(preds, axis = 1)
preds